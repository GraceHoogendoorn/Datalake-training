{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3TwAK_8Wll0"
   },
   "source": [
    "## 3. Data Scientist - Create ML models with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rwCbd6SqWll0"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, LogisticRegression\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    ".appName('Spark - Data Scientist Demo') \\\n",
    ".config('spark.jars', '/usr/lib/spark/jars/spark-bigquery-latest.jar') \\\n",
    ".config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.18.0\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.get(\"spark.app.id\")\n",
    "spark.sparkContext._jvm.scala.util.Properties.versionString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "bq_raw_dataset_name = project_id[0] + '-raw'\n",
    "bq_raw_dataset_name = bq_raw_dataset_name.replace('-', '_')\n",
    "bq_raw_table_path = project_id[0] + ':' + bq_raw_dataset_name + '.transaction_data_train' \n",
    "bq_raw_table_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Training Data using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transaction_data_train = spark.read \\\n",
    ".format(\"bigquery\") \\\n",
    ".option(\"table\", bq_raw_table_path) \\\n",
    ".load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_name = \"bank_transaction_view\"\n",
    "df_transaction_data_train.createOrReplaceTempView(view_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.sql(\"\"\"\n",
    "SELECT * \n",
    "FROM bank_transaction_view\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop('transactionID')\n",
    "data.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a pyspark ML pipeline \n",
    "\n",
    "The pipeline will transform the features and train a Decision Tree classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier, DecisionTreeClassifier\n",
    "\n",
    "\n",
    "categorical_cols = [field for (field, data_type) in data.dtypes \n",
    "                    if ((data_type == \"string\") & (field != 'isFraud'))]\n",
    "\n",
    "ohe_output_cols = [x + \"_OHE\" for x in categorical_cols]\n",
    "\n",
    "string_indexers = StringIndexer(inputCol='type', outputCol='type' +\"_Index\").fit(data) \n",
    "\n",
    "one_hot_indexer = OneHotEncoder(inputCol='type_Index', outputCol='type' +\"_OHE\")\n",
    "\n",
    "numeric_cols = [field for (field, data_type) in data.dtypes \n",
    "                if (((data_type == \"double\") | (data_type == \"int\") | (data_type == \"bigint\"))\n",
    "                  & (field != 'isFraud'))]\n",
    "\n",
    "assembler_inputs = ohe_output_cols + numeric_cols\n",
    "\n",
    "vec_assembler = VectorAssembler(\n",
    "    inputCols=assembler_inputs,\n",
    "    outputCol=\"features\")\n",
    "\n",
    "\n",
    "dtc = DecisionTreeClassifier(labelCol=\"isFraud\", featuresCol=\"features\", maxDepth=3, maxBins=12)\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    string_indexers,\n",
    "    one_hot_indexer,\n",
    "    vec_assembler,\n",
    "    dtc \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Persist the model to GCS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "\n",
    "gcs_bucket = project_id[0] + '-data'\n",
    "model_path = f'gs://{gcs_bucket}/model/'\n",
    "\n",
    "model.write().overwrite().save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict on test data \n",
    "**TODO**\n",
    "* Provide path_to_predict_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_predict_csv = \"<gcs-path>/transaction_data_test.csv\"\n",
    "df_transaction_data_predict_from_csv = spark \\\n",
    ".read \\\n",
    ".option(\"inferSchema\" , \"true\") \\\n",
    ".option(\"header\" , \"true\") \\\n",
    ".csv(path_to_predict_csv)\n",
    "df_transaction_data_predict_from_csv.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the saved model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_pipeline_model = PipelineModel.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = loaded_pipeline_model.transform(df_transaction_data_predict_from_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"isFraud\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "binaryEvaluator = BinaryClassificationEvaluator(labelCol=\"isFraud\")\n",
    "\n",
    "auc = binaryEvaluator.evaluate(predictions, {binaryEvaluator.metricName: \"areaUnderROC\"})\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests_np = np.array((predictions.select(\"isFraud\",\"prediction\").collect()))\n",
    "tests_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests_np = np.array((predictions.select(\"isFraud\",\"prediction\").collect()))\n",
    "\n",
    "np_acc = accuracy_score(tests_np[:,0], tests_np[:,1])\n",
    "np_f1 = f1_score(tests_np[:,0], tests_np[:,1])\n",
    "np_precision = precision_score(tests_np[:,0], tests_np[:,1])\n",
    "np_recall = recall_score(tests_np[:,0], tests_np[:,1])\n",
    "np_auc = roc_auc_score(tests_np[:,0], tests_np[:,1])\n",
    "\n",
    "print(\"f1:\", np_f1)\n",
    "print(\"precision:\", np_precision)\n",
    "print(\"recall:\", np_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import package that will generate the confusion matrix scores\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# import packages that will help display the scores\n",
    "import pandas as pd\n",
    "\n",
    "confusion_matrix_scores = confusion_matrix(tests_np[:,0], \n",
    "                                           tests_np[:,1], \n",
    "                                           labels=[1, 0])\n",
    "\n",
    "# display scores as a heatmap\n",
    "df = pd.DataFrame(confusion_matrix_scores, \n",
    "                  columns = [\"Predicted True\", \"Predicted Not True\"],\n",
    "                  index = [\"Actually True\", \"Actually Not True\"])\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_annotated_table_name = 'transaction_data_predictions'\n",
    "bq_annotated_table_path=  project_id[0] +  '_annotated.' + bq_annotated_table_name\n",
    "bq_annotated_table_path = bq_annotated_table_path.replace('-', '_')\n",
    "bq_annotated_table_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Persist predictions as an annotated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_inline = predictions.schema.simpleString().replace('struct<', '').replace('>', '').replace('int', 'int64').replace('double', 'float64').replace('bigint64', 'int64').replace('vector', 'STRING')\n",
    "\n",
    "!bq mk --table \\\n",
    "{bq_annotated_table_path} \\\n",
    "{schema_inline}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.write \\\n",
    ".format(\"bigquery\") \\\n",
    ".option(\"table\", project_id[0]  + ':' + bq_annotated_table_path) \\\n",
    ".option(\"temporaryGcsBucket\", project_id[0]  + '-data') \\\n",
    ".mode('overwrite') \\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_dataset_name =  project_id[0] +  '_annotated'\n",
    "annotated_dataset_name = annotated_dataset_name.replace('-', '_')\n",
    "annotated_dataset_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** \n",
    "* Add annotated_dataset_name in the FROM clause below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery\n",
    "SELECT * FROM <annotated_dataset_name>.INFORMATION_SCHEMA.TABLES;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join buisness data to enrich the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** \n",
    "* Provide the path to the join csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_join_csv = \"gs://<enriched_dataset_name>/transaction_data_join.csv\"\n",
    "df_transaction_data_join_from_csv = spark \\\n",
    ".read \\\n",
    ".option(\"inferSchema\" , \"true\") \\\n",
    ".option(\"header\" , \"true\") \\\n",
    ".csv(path_to_join_csv)\n",
    "df_transaction_data_join_from_csv.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** (Challenge 2)\n",
    "* Join the 2 spark dataframes (predictions & df_transaction_data_join_from_csv) on transactionID field "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_result = predictions.join(df_transaction_data_join_from_csv, \"transactionID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_result.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_result.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Persist result as an enriched dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_enriched_table_name = 'transaction_analysis_enriched'\n",
    "bq_enriched_table_path = project_id[0] +  '_enriched.' + bq_enriched_table_name\n",
    "bq_enriched_table_path = bq_enriched_table_path.replace('-', '_')\n",
    "bq_enriched_table_path = project_id[0] + ':' + bq_enriched_table_path\n",
    "bq_enriched_table_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_inline = joined_result.schema.simpleString().replace('struct<', '').replace('>', '').replace('int', 'int64').replace('bigint64', 'int64').replace('double', 'float64').replace('vector', 'STRING')\n",
    "\n",
    "!bq mk --table \\\n",
    "{bq_enriched_table_path} \\\n",
    "{schema_inline}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_result.write \\\n",
    ".format(\"bigquery\") \\\n",
    ".option(\"table\", bq_enriched_table_path) \\\n",
    ".option(\"temporaryGcsBucket\", project_id[0]  + '-data') \\\n",
    ".mode('overwrite') \\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_dataset_name = project_id[0] +  '_enriched'\n",
    "enriched_dataset_name = enriched_dataset_name.replace('-', '_')\n",
    "enriched_dataset_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**\n",
    "* Provide the enriched_dataset_name in the FROM clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery\n",
    "SELECT * FROM <enriched_dataset_name>.INFORMATION_SCHEMA.TABLES;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**\n",
    "* Query the enriched table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery \n",
    "<inser-code-here>\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** (Optional: Challenge 3)\n",
    "* Improve the ML pipeline\n",
    "    * Try out different ML models [[doc]](https://spark.apache.org/docs/latest/ml-pipeline.html)\n",
    "    * Explore hyperparameter tuning \n",
    "    * How would you split the data when there is class imbalance? "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "test-1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
