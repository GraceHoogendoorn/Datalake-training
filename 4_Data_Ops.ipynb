{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1-2NGnBscJo"
   },
   "source": [
    "## 5. Data Ops - Deploy Spark pipeline using Dataproc Workflows\n",
    "\n",
    "### Dataproc Workflows\n",
    "\n",
    "Dataproc Workflows has 2 types of workflow templates.\n",
    "\n",
    "1. Manged cluster - Create a new cluster and delete the cluster once the job has completed.\n",
    "2. Cluster selector - Select a pre-existing Dataproc cluster to the run the jobs (does not delete the cluster).\n",
    "\n",
    "This codelab will use option 1 to create a managed cluster workflow template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYHGVNfTscJo"
   },
   "source": [
    "### 5.1 Convert code above into 2 python files\n",
    "\n",
    "1. Job to convert CSV to BQ Tables\n",
    "2. Job to run predictions by reading bank marketing data from hive table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datalake-vol2'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_id = !gcloud config list --format 'value(core.project)' 2>/dev/null \n",
    "project_id = project_id[0]\n",
    "project_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "DCQwAUOfscJp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting job_csv_to_bq_table.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile job_csv_to_bq_table.py\n",
    "## Job 1\n",
    "print('Job 1')\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    ".appName('Spark - Data Eng Demo') \\\n",
    ".config('spark.jars', 'gs://spark-lib/bigquery/spark-bigquery-latest.jar') \\\n",
    ".getOrCreate()\n",
    "\n",
    "project_id = 'datalake-vol2'\n",
    "# create temp GCS bucket for writing spark df to bq table\n",
    "gcs_bucket = project_id + '-data'\n",
    "# create the name your BQ dataset\n",
    "dataset_name = project_id + '-raw'\n",
    "dataset_name = dataset_name.replace('-', '_')\n",
    "table_name = dataset_name + \".transaction_data_aut\"\n",
    "\n",
    "sql_statement = f\"\"\"CREATE DATABASE IF NOT EXISTS {dataset_name};\"\"\"\n",
    "\n",
    "spark.sql(sql_statement)\n",
    "\n",
    "df_transaction_data_from_csv = spark \\\n",
    "  .read \\\n",
    "  .option ( \"inferSchema\" , \"true\" ) \\\n",
    "  .option ( \"header\" , \"true\" ) \\\n",
    "  .csv (\"gs://datalake-vol2-data/dataset/transaction_data_train.csv\" )\n",
    "\n",
    "df_transaction_data_from_csv.write \\\n",
    ".format(\"bigquery\") \\\n",
    ".option(\"table\", table_name) \\\n",
    ".option(\"temporaryGcsBucket\", gcs_bucket) \\\n",
    ".mode('overwrite') \\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pmWJ9x0bscJp"
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "df-wxP3sscJq"
   },
   "outputs": [],
   "source": [
    "# %%writefile job_xgboost_predictions.py\n",
    "# ## Job 2\n",
    "# print('Job 2')\n",
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.ml import Pipeline, PipelineModel\n",
    "# from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# warehouse_location = 'gs://cloud-native-dl-modernisation-demo/hive-warehouse'\n",
    "# service_endpoint = 'thrift://hive-cluster-m.us-central1-c:9083'\n",
    "\n",
    "# spark = SparkSession.builder \\\n",
    "#   .appName('csv_to_hive') \\\n",
    "#   .config(\"hive.metastore.uris\", service_endpoint)  \\\n",
    "#   .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "#   .enableHiveSupport() \\\n",
    "#   .getOrCreate()\n",
    "\n",
    "# # Load the data\n",
    "# data = spark.sql(\"\"\"\n",
    "# SELECT * \n",
    "# FROM bank_demo_db.bank_marketing\n",
    "# \"\"\")\n",
    "\n",
    "# (train_data, test_data) = data.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# model_path = 'gs://cloud-native-dl-modernisation-demo/models/xgboost/pipeline_model/bank-marketing'\n",
    "\n",
    "# loaded_pipeline_model = PipelineModel.load(model_path)\n",
    "# predictions = loaded_pipeline_model.transform(test_data)\n",
    "\n",
    "# # save predictions as json\n",
    "# predictions.write.json('gs://cloud-native-dl-modernisation-demo/models/xgboost/predictions_json/'+spark.sparkContext.applicationId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pUbyQ3ZdscJq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2fL7Z5iscJr"
   },
   "source": [
    "### 5.2. Grant service account permission to deploy workflow from notebooks\n",
    "\n",
    "Dataproc's service accounts needs to be granted \"Dataproc Editor\" IAM role.\n",
    "\n",
    "Go to https://console.cloud.google.com/iam-admin/iam\n",
    "\n",
    "Look for the service account email under the name \"Google Cloud Dataproc Service Agent\". It will be in the format\n",
    "\n",
    "```bash\n",
    "service-{project-number}@dataproc-accounts.iam.gserviceaccount.com\n",
    "```\n",
    "\n",
    "Edit the roles and add the role \"Dataproc Editor\" and press save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ykssh-k9scJs"
   },
   "outputs": [],
   "source": [
    "## Alternatively run all of the below from the Google Cloud Shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QC7qXjdMscJt"
   },
   "source": [
    "### 5.3 Create Dataproc managed cluster workflow Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WORKFLOW_ID=read-csv-to-bq-workflow\n",
      "env: REGION=europe-west3\n"
     ]
    }
   ],
   "source": [
    "%env WORKFLOW_ID=read-csv-to-bq-workflow \n",
    "%env REGION=europe-west3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "echo $REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GUQ7vcnuscJw"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud dataproc workflow-templates create $WORKFLOW_ID \\\n",
    "--region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8BqGX5VscJw"
   },
   "source": [
    "### 5.4 Configure managed cluster for the workflow template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YVl2u2mSscJx"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "export PROJECT_ID=datalake-vol2\n",
    "export CLUSTER_NAME=spark-workflow-cluster\n",
    "export BUCKET_NAME=${PROJECT_ID}-data\n",
    "export REGION=europe-west3\n",
    "export NUM_WORKERS=2\n",
    "\n",
    "gcloud beta dataproc workflow-templates set-managed-cluster $WORKFLOW_ID \\\n",
    "    --cluster-name $CLUSTER_NAME \\\n",
    "    --region $REGION \\\n",
    "    --image-version=1.5-ubuntu18 \\\n",
    "    --master-machine-type n1-standard-4 \\\n",
    "    --num-workers $NUM_WORKERS \\\n",
    "    --worker-machine-type n1-standard-4\\\n",
    "    --scopes https://www.googleapis.com/auth/cloud-platform \\\n",
    "    --bucket $BUCKET_NAME \\\n",
    "    --enable-component-gateway \\\n",
    "    --properties spark:spark.jars=gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.18.0.jar,spark:spark=gs://spark-lib/bigquery/spark-bigquery-latest.jar\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZG53bc48scJy"
   },
   "source": [
    "### 5.5 Upload PySpark job to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "_O0wO8H7scJz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://job_csv_to_bq_table.py [Content-Type=text/x-python]...\n",
      "/ [1 files][  1.0 KiB/  1.0 KiB]                                                \n",
      "Operation completed over 1 objects/1.0 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export PROJECT_ID=datalake-vol2\n",
    "export BUCKET_NAME=${PROJECT_ID}-data\n",
    "gsutil cp job_csv_to_bq_table.py \\\n",
    " gs://${PROJECT_ID}-data/workflows/python-scripts/job_csv_to_bq_table.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_ppc1GfscJz"
   },
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# export PROJECT_ID=cloud-native-dl-modernisation\n",
    "# export BUCKET_NAME=${PROJECT_ID}-demo\n",
    "# gsutil cp job_xgboost_predictions.py \\\n",
    "#  gs://${PROJECT_ID}-demo/workflows/spark-bank-marketing/job_xgboost_predictions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DX87wdpscJ0"
   },
   "source": [
    "### 5.6 Add job to workflow template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "lukmzJbMscJ0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: (gcloud.dataproc.workflow-templates.add-job.pyspark) INVALID_ARGUMENT: Template contains a duplicate job with step name 'csv_to_bq'\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'export WORKFLOW_ID=read-csv-to-bq-workflow\\nexport REGION=europe-west3\\nexport PROJECT_ID=datalake-vol2\\n\\ngcloud dataproc workflow-templates add-job pyspark \\\\\\n   gs://${PROJECT_ID}-data/workflows/python-scripts/job_csv_to_bq_table.py \\\\\\n    --region $REGION \\\\\\n    --step-id csv_to_bq \\\\\\n    --workflow-template $WORKFLOW_ID\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-e1ca952a92e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'export WORKFLOW_ID=read-csv-to-bq-workflow\\nexport REGION=europe-west3\\nexport PROJECT_ID=datalake-vol2\\n\\ngcloud dataproc workflow-templates add-job pyspark \\\\\\n   gs://${PROJECT_ID}-data/workflows/python-scripts/job_csv_to_bq_table.py \\\\\\n    --region $REGION \\\\\\n    --step-id csv_to_bq \\\\\\n    --workflow-template $WORKFLOW_ID\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/miniconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2380\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2381\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2382\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2383\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/miniconda3/lib/python3.8/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-103>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/miniconda3/lib/python3.8/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/miniconda3/lib/python3.8/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'export WORKFLOW_ID=read-csv-to-bq-workflow\\nexport REGION=europe-west3\\nexport PROJECT_ID=datalake-vol2\\n\\ngcloud dataproc workflow-templates add-job pyspark \\\\\\n   gs://${PROJECT_ID}-data/workflows/python-scripts/job_csv_to_bq_table.py \\\\\\n    --region $REGION \\\\\\n    --step-id csv_to_bq \\\\\\n    --workflow-template $WORKFLOW_ID\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export WORKFLOW_ID=read-csv-to-bq-workflow\n",
    "export REGION=europe-west3\n",
    "export PROJECT_ID=datalake-vol2\n",
    "\n",
    "gcloud dataproc workflow-templates add-job pyspark \\\n",
    "   gs://${PROJECT_ID}-data/workflows/python-scripts/job_csv_to_bq_table.py \\\n",
    "    --region $REGION \\\n",
    "    --step-id csv_to_bq \\\n",
    "    --workflow-template $WORKFLOW_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sdtd-QZpscJ1"
   },
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# export WORKFLOW_ID=read-csv-to-bq-workflow\n",
    "# export REGION=europe-west3\n",
    "# export PROJECT_ID=datalake-vol2\n",
    "\n",
    "# gcloud dataproc workflow-templates add-job pyspark \\\n",
    "#   gs://${PROJECT_ID}-demo/workflows/spark-bank-marketing/job_xgboost_predictions.py \\\n",
    "#     --region $REGION \\\n",
    "#     --start-after=csv_to_hive \\\n",
    "#     --step-id xgboost_predictions \\\n",
    "#     --workflow-template $WORKFLOW_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ug2dKqWscJ2"
   },
   "source": [
    "### 5.7 Run workflow template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "d5UY8epDscJ2",
    "outputId": "5d09224f-4790-4ece-f689-23d604017baf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting on operation [projects/datalake-vol2/regions/europe-west3/operations/9b7b17a5-84ba-3c52-b1a3-6b470789a1df].\n",
      "WorkflowTemplate [read-csv-to-bq-workflow] RUNNING\n",
      "Creating cluster: Operation ID [projects/datalake-vol2/regions/europe-west3/operations/c5b200dd-d5f6-48b0-9021-0d782cf10b21].\n",
      "Created cluster: spark-workflow-cluster-4ptd3p6o2gf3e.\n",
      "Job ID csv_to_bq-4ptd3p6o2gf3e RUNNING\n",
      "Job ID csv_to_bq-4ptd3p6o2gf3e FAILED\n",
      "Job ID csv_to_bq-4ptd3p6o2gf3e error: Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found at:\n",
      "https://console.cloud.google.com/dataproc/jobs/csv_to_bq-4ptd3p6o2gf3e?project=datalake-vol2&region=europe-west3\n",
      "gcloud dataproc jobs wait 'csv_to_bq-4ptd3p6o2gf3e' --region 'europe-west3' --project 'datalake-vol2'\n",
      "https://console.cloud.google.com/storage/browser/datalake-vol2-data/google-cloud-dataproc-metainfo/24acf9f0-7f5d-4dac-8e9d-94c401caa3f0/jobs/csv_to_bq-4ptd3p6o2gf3e/\n",
      "gs://datalake-vol2-data/google-cloud-dataproc-metainfo/24acf9f0-7f5d-4dac-8e9d-94c401caa3f0/jobs/csv_to_bq-4ptd3p6o2gf3e/driveroutput\n",
      "Deleting cluster: Operation ID [projects/datalake-vol2/regions/europe-west3/operations/f0661ed4-a014-471c-b1fd-4e72eaf97a64].\n",
      "WorkflowTemplate [read-csv-to-bq-workflow] DONE\n",
      "Deleted cluster: spark-workflow-cluster-4ptd3p6o2gf3e.\n",
      "ERROR: (gcloud.dataproc.workflow-templates.instantiate) Operation [projects/datalake-vol2/regions/europe-west3/operations/9b7b17a5-84ba-3c52-b1a3-6b470789a1df] failed: Step 'csv_to_bq' (csv_to_bq-4ptd3p6o2gf3e) failed.\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'\\ngcloud dataproc workflow-templates instantiate $WORKFLOW_ID \\\\\\n--region $REGION\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-6444fe13f1b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\ngcloud dataproc workflow-templates instantiate $WORKFLOW_ID \\\\\\n--region $REGION\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/miniconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2380\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2381\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2382\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2383\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/miniconda3/lib/python3.8/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-103>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/miniconda3/lib/python3.8/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/miniconda3/lib/python3.8/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'\\ngcloud dataproc workflow-templates instantiate $WORKFLOW_ID \\\\\\n--region $REGION\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "gcloud dataproc workflow-templates instantiate $WORKFLOW_ID \\\n",
    "--region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "owS3gXdHscJ4"
   },
   "source": [
    "### 5.8 View Cluster, workflow and jobs tabs\n",
    "\n",
    "Go to the Dataproc UI and view the cluster page. You should see the new cluster spinning up\n",
    "\n",
    "Once the cluster is ready view the workflow and jobs tabs to check the progress of the jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fg9asdqhscJ5"
   },
   "source": [
    "### 5.9 Check new predictions table was created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDhN2ph6scJ6"
   },
   "source": [
    "### 5.10 Schedule workflows\n",
    "\n",
    "View the guide on how to schedule Dataproc workflows\n",
    "\n",
    "https://cloud.google.com/dataproc/docs/concepts/workflows/workflow-schedule-solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YatpAcsJscJ8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Spark - Bank Marketing Demo (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
